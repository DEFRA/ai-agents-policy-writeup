# Retro on the ai-sdlc-agent-poc 

## Spike #1 
Parallel execution with Tools 

**Issues**: 
- The implementation is fairly complex - a lot of immutable shared state being used by each branch e.g. repository_url and ingested_repository leading to lots of reducers and config object
- More importantly, we got errors calling the anthropic apis - "Too many requests" and "token limit exceeded"

**Positives**
- Factory pattern - nice and reusable. Going forward - use subgraphs, reusable ones.
- We got tools working well
- We learnt lessons around the importance of state management and reducers when trying to do parallel


**Lessons** **learnt**: 
- This solution requires a lot of "context" - the source code - which it passes into the model calls
- We managed to get it parallel graphs working, but they caused issues calling the anthropic apis due to all this context - this pattern is probably better suited for other, less token heavy, applications
- When designing LangGraphs it's crucial to design state and reducers first and feed that into the reqs and prompts

**Alternatives**: 
1. Use a sequential process - less concurrent cals = less likely to hit  "Too many requests" and "token limit exceeded"
2. Use RAG instead of tools - tools are inherently more chatty 
3. Work on the prompts to keep them concise and to the point. Also maybe use xml or json instead of markdown for the interim steps e.g. data model analysis, until we get to the final human readable report