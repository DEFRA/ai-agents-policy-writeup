- I (Ben) had an interesting conversation with Justin Coyne yesterday (Friday) - he's mapped out all the steps he can think of in the end to end SDLC in Government / Defra and he's keen to feed into the playbook about how we can cover a lot of the gaps. Unsurprisingly the gaps are to left of development and a lot around UCD and Delivery. TBD when Anne & I are back.
- I (Ben) want to do another refresh/refactor of the prompt library.  I think some of the current prompts are out of date and too specific to use cases we've worked on in the past. I've also listed below some prompts I'd like to add. 

## User Story Creation - a simpler example using non-thinking model like 4o
```
You are a senior business analyst working in a software delivery team in UK Government.

Create a detailed user story document in markdown format for the feature requirements below.

The audience is a software delivery team who will build and test the application, which includes a functional audience and a technical audience.

Produce the user story as a markdown file. Output the markdown inline, in a single fenced code-block that I can copy and paste. Ensure you escape inline code blocks correctly. 

Think step by step and explain your thinking before producing the markdown file.

# User Story Format

The format and content of the user story must be as follows:

User Story:
[User story summary in "AS A, I WANT, SO THAT" format. This must be functional and must omit technical details]

Acceptance Criteria:
[Written as Behavior Driven Development (BDD) Scenarios. The BDD scenarios should focus more on functional/user-driven actions. Omit technical details. Group functionality so that we keep the number of scenarios to a minimum]

Interface Design:
[Relevant user interface design. This is a GOV.UK Application so it should follow GOV.UK guidelines including the GDS Design System and Accessibility. Ensure you include the GDS Components needed for the interface]

Technical Design:
[The functionality defined in technical detail.  Include the API Responses as examples from below]

---

# Context

[insert context]

# Detailed Requirements

[insert detailed requirements]

# Verification Checklist
- The user story contains format and content as defined above. It does not need to have any additional sections.

- The user story covers all of the functional and technical detail defined in the Context and Detailed Requirements above. It does not need to have any additional technical details.  

- The user story as a markdown file. Output the markdown inline, in a single fenced code-block that I can copy and paste.
```


## Add a New Test - in Jest (non TDD)

```
# CONTEXT

I have provided you with a Product Requirements Document (PRD). Please analyze feature [FEATURE_NAME] and its User Stories to create comprehensive Jest tests. Do not work on other features yet.

## ANALYSIS PHASE

Before writing tests, please analyze:
- The specific feature and associated user stories in the PRD
- The relevant source code components and their interactions
- Integration points with other components/services
- External dependencies requiring mocks (APIs, services, databases)
- Data models, state management, and user data flows
- Existing test coverage and testing patterns used in the codebase

## IMPLEMENTATION PHASE

### Testing Philosophy:
- Focus on user-visible behavior and outcomes rather than implementation details
- Test both server-rendered content and client-side interactive behaviors
- Verify functionality from a user's perspective (what users see and experience)
- Test component contracts and interfaces, not internal workings
- Mock external dependencies like APIs, but use realistic mock data

### Test Types to Include:
- Functional tests that verify user workflows
- Component rendering tests (snapshot or DOM verification)
- Interactive behavior tests (user actions, form submissions)
- Error handling and edge case tests
- Accessibility tests where applicable

### Test Structure:
- Use Given-When-Then pattern with descriptive comments
- Maintain clarity by using descriptive test names
- Group related tests logically using nested `describe` blocks
- Use setup and teardown hooks consistently (`beforeEach`, `afterEach`)
- Isolate tests to prevent cross-test dependencies

## CONSTRAINTS

- Scope: Modify only the test files, not the source code
- Format: Deliver complete, production-ready test code
- Coverage: Aim for comprehensive coverage of user scenarios rather than 100% code coverage
- Performance: Ensure tests run efficiently and don't cause unnecessary re-renders

## VERIFICATION PHASE

Please provide:
1. A summary of your analysis of the feature
2. The complete test implementation with explanatory comments
3. An execution report for the tests
4. Any issues encountered and their solutions
5. Recommendations for improving testability (if applicable)
```

## TDD in Claude 4 (as per blog)

```
I have provided you with a Product Requirements document(PRD). Please analyze the features and User Stories and implement Feature 1. 

I want to use strict test-driven development. Please write the failing test first, then we'll write code to make it pass. Do not write any implementation code until I've reviewed the test.

Follow the Red-Green-Refactor cycle strictly:
1. RED: Write a failing test
2. GREEN: Minimal code to pass
3. REFACTOR: Improve while keeping tests green

Start with step 1 only

Write failing tests for Feature 1. Show me:
1. The test code
2. Why it will fail
3. What error message we'd expect

Test-First Reasoning:
Before writing any implementation, walk me through:
- What behavior should this test verify?
- What would make this test pass?
- What assumptions am I making?
```

```
Now lets do step 2. GREEN: Minimal code to pass

Writing Minimal Code
Now write the absolute minimum code needed to make this test pass. Nothing more, nothing less.
Make this test pass with the simplest possible implementation. Don't worry about edge cases or future features yet.

Catching Over-Engineering
Is this the minimum code needed, or are you implementing extra features not covered by the current test?
Review your implementation: What parts aren't required by the failing test? Remove everything unnecessary.

After please execute the tests
```

```
Now lets do step 3. REFACTOR: Improve while keeping tests green

Refactor this code to improve the readability and structure while keeping all tests green. Show me the refactored version and confirm tests still pass
```