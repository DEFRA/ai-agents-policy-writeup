# Ben / Todd discussion about future team shape for next project

- Create the role of Generalist (role that intentionally goes across all roles in a team), but still allows for specialist roles if an individual wishes
- Have a smaller team comprised of both generalists and specialists (Rather than the usual 10 person team, maybe 5-7.  Odd number important to break ties)
- Internally the team may want to split up into small 3 person squads to accomplish tasks.
- Generally, the internal team can structure themselves as they please, however, there should be really clear interfaces into and out of the team from the perspective of governance, business requirements, user feedback, operational interfaces, etc.  How those requests are dealt with is up to the team (may not necessarily be the same individual fielding the requests)
- Next project should be user facing, small but impactful.  Something AI tooling can really accelerate

# Todd's notes on product management workflows

- use the 03-mini model - very impressive
- Asking it to ask clarifying questions puts it into a sort of 'deep research' mode - really asks some insightful questions.
- I used an obsidian plugin called 'Share as Gist' to share an Obsidian md file to a public gist, which I could then use to start a fresh session with o1 to do some more deep analysis on the product design, cost-benefit, etc
- Would be cool to come up with an AI product design workflow for the playbook, where you can pretty much create a business case for each feature.  Things I was asking:
	- Create a mockup (pretty much the first thing) *Look into using AI tools to create interface mockups*
	- Creating a data model (pretty much the second thing)
	- Ask o3 to do a meta analysis on the product feature: ask if it understands the feature in it's own words, suggest pros/cons, recommendations for improvements and alternatives
	- Ask to do a cost / benefit analysis
	- Ask to do a futurespective on possible issues if people use the feature long term
	- Upload a picture of the interface using 01 and the data model, ask if the data model supports the interface functionality and how
	- Workflow (with prompts if we develop it) should aim to ask good questions and remove overly optimistic bias.

## Todd's prompt tweak to cut stories out of product requirements promts (alternative to PRDs - based on Ben's prompt)

```
Read and understand these product requirements below. 

Convert it into multiple user stories, in markdown format. 

I want it to be concise but thorough. But it must cover all the functionality defined in the product requirements, so double check that all the functionality listed is covered.

I want the format of the user stories to be 

---
User story summary in "AS A, I WANT, SO THAT" format

Interface Design (if applicable):
Detailing any interface design elements needed using GDS design standards and GOV.UK components.

Acceptance Criteria: 
Written as BBD Scenarios. Note the BDD Scenarios should be functional and omit technical details

Technical Design:
The functionality defined in technical detail. As per functional design in the PRD below. Also include the API Responses as examples from below

---

Product Requirements:
[REQUIREMETNS TEXT HERE]
```



**WIP - Prompt for agent tests**

- Testing standards reference: .cursor/rules/02_testing.mdc

```
Create unit tests for the [agent name]

## Context & Requirements
- Primary function to test: `analyze_codebase_classifications` in `@standards_classification_agent.py`
- Only test the public interface, not internal functions
- Mock external dependencies: `AnthropicClient`, file system, etc  
- Do not mock internal modules, classes or functions

## Test Structure Guidelines
1. Test Organization:
   - Follow Given-When-Then pattern with inline comments
   - Group tests by scenarios (happy path, error cases, edge cases)
   - Use descriptive test names that indicate the scenario being tested

2. Test Coverage Requirements:
   - Core user scenarios and workflows
   - Edge cases and error conditions
   - Both happy and unhappy paths

3. Mocking Strategy:
   - Mock AnthropicClient at module level
   - Mock file operations where needed
   - Ensure mocks return appropriate responses for each scenario

4. Test Data:
   - Provide realistic test data that matches production patterns
   - Include sample codebase content for testing

## Implementation Notes
- Use pytest fixtures for common setup
- Follow existing test patterns in the codebase
- Maintain minimum 90% code coverage
- Test through the public interface only
```

Notes - this worked well for the simple standards_classification_agent, but not for the more complex stadnards agent. Need to work on this - perhaps an integration test rather than a unit test?